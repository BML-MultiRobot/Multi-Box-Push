    policyPars = {
                'neurons':      (10, 250, 250, 250, 8),  # 5 box-related state, 4 goal-related state, 8 controls
                'act':          ['F.leaky_relu', 'F.leaky_relu', 'F.leaky_relu'],
                'mu':           torch.zeros(10),
                'std':          torch.ones(10),
                'trainMode':    False,  # Make sure both value and policy are set to the same thing
                'load':         False, 
                'dual':         False,
                'beta':         12  # boltzmann. Increase for more certainty when making decisions
                } 
    policyTrain = {
                'batch':        256,  # used to be 256
                'lr':           3e-4, 
                'buffer':       5000,
                'gamma':        .975,
                'explore':      0,  # Don't change this. Tune the exploration up top
                'double':       True,
                'noise':        0,
                'priority':     False
                }

        def reward_function(self, s):
        s = s.ravel()
        succeeded = self.succeeded(s)
        done = self.decide_to_restart(s)

        if succeeded:
            if self.simulation_name == 'elevated_scene':
                return 5 - dist(s[:2], s[5:7]) * 5
            if self.simulation_name == 'flat_scene':
                return 5 - abs(self.box_ori_global) * 3
            if self.simulation_name == 'slope_scene':
                return 5 - abs(self.box_ori_global) * 3
        if done and not succeeded:
            return -5
        else:
            if type(self.prev["S"]) != np.ndarray:
                return 0
            previous_local_state = self.prev['S'].ravel()
            previous_distance = dist(previous_local_state[:3], previous_local_state[5: 8])
            curr_distance = dist(s[:3], s[5: 8])
            d_reward = previous_distance - curr_distance

            prev_ori = abs(previous_local_state[4] - self.get_goal_angle(previous_local_state))
            curr_ori = abs(s[4] - self.get_goal_angle(s))
            ori_reward = prev_ori - curr_ori if curr_distance > 1 else 0  # if to prevent overshoot

        
            return 5 * np.round(.75 * d_reward + .25 * ori_reward, 2) - .2

exploration capped at .2

        self.travel_gain = 2.5  
        self.align_gain = 3
        self.rotate_gain = 3
        self.x_contact = 0
        self.contact = {'left': .6, 'right': -.6}


