    def reward_function(self, s, a, s_prime):
        """ Reward Function using global states and actions """
        s, s_prime = s.flatten(), s_prime.flatten()

        locations = s[:-1]
        squared = np.square(locations)

        squared_distances = squared[::2] + squared[1::2]
        prev_distance = np.sum(np.sqrt(squared_distances))

        locations = s_prime[:-1]
        squared = np.square(locations)
        squared_distances = squared[::2] + squared[1::2]
        new_distance = np.sum(np.sqrt(squared_distances))

        r_agent_distance = prev_distance - new_distance
        # TODO: This is a test
        r_ball_forward = self.ball_location[0] - self.prev_ball[0]
        r_ball_side = abs(self.prev_ball[1]) - abs(self.ball_location[1])
        return r_ball_forward * 5 + r_ball_side * 5  # + r_agent_distance

    params = {
              # general parameters
              'max_ep_len': 100, 'explore_steps': 1000, 'test_mode': False,

              # reward network
              'reward_width': 300, 'reward_depth': 3, 'reward_lr': 3e-4,
              'reward_batch': 200, 'rotation_invariance': True,
              'noise_std': 0, 'reward_weight': 8, 'reward_epochs': 5,

              # General Policy parameters
              'policy_batch': 250, 'a': 5e-2,  # a = lr for q learn policy gradient
              'load_policy': None, # '/home/jimmy/Documents/Research/CLA/results/off_policy_IQL/policy.pickle',  # '/home/jimmy/Documents/Research/CLA/results/off_policy_cf_ddac/policy.pickle',
              'movement_constraint': True,  # Toggles whether we constrain action space with if statements
              'policy_steps': 10, 'policy_buffer_size': 10000,

              # diff-q policy gradient parameters
              'q_lr': .05, 'gamma': .9,
              'steps_per_train': 1, 'counterfactual': True, 'distribution_buff': False,
              'distribution_hold': 15, 'dist_lr': .4, 'dist_weight': .1,

              # control parameters
              # 'rim_size': .02,
              'rim_size': .075, 'near_ball': .325,

              # exploration
              'explore': .1
              }
BOX MASS = .01

def to_state_id(num_bots, locations, orientations, ball_location, near_ball):
    """ Generates local, discrete state for robot number {robot_id}

        Indicators: near ball (2), ball location (4), y sign (3), on left (2), on right(2), front(2), back (2)
        Total: 256 states. Lots of them. But, we have lots of robots.

        Simpler: near ball (2), ball location (8), y sign (3)
        Total: 48
        """
    features = {i: [] for i in range(num_bots)}
    state_ids = {}

    location_indicators, allowed = {}, {}
    occupied = [False for _ in range(16)]  # Indicators for different positions around ball
    for robot_id in range(num_bots):
        robot_location, robot_ori = locations[robot_id], orientations[robot_id]
        near_ball_indicator = int(distance(robot_location, ball_location) < near_ball)
        state_location_indicator, actual_location_indicator = determine_direction(robot_location, robot_ori, ball_location)
        occupied[actual_location_indicator] = True
        location_indicators[robot_id] = actual_location_indicator
        ball_y_indicator = 0 if ball_location[1] > 0.05 else (1 if ball_location[1] < -.05 else 2)
        features[robot_id] = [state_location_indicator, near_ball_indicator, ball_y_indicator]

    for robot_id in range(num_bots):
        f = features[robot_id]
        loc = location_indicators[robot_id]
        clockwise = (loc - 1) % 16
        c_clockwise = (loc + 1) % 16
        f.append(occupied[clockwise])
        f.append(occupied[c_clockwise])
        allowed[robot_id] = {'c': not occupied[clockwise], 'cc': not occupied[c_clockwise]}

        state_id = 0
        for i, f in enumerate(f):
            state_id += f * (100 ** i)
        state_ids[robot_id] = state_id

    return state_ids, location_indicators, allowed

number of agents = 10, 16 subsections
