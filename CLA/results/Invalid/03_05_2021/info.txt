    def reward_function(self, s, a, s_prime):
        """ Reward Function using global states and actions """
        s, s_prime = s.flatten(), s_prime.flatten()

        locations = s[:-1]
        squared = np.square(locations)

        squared_distances = squared[::2] + squared[1::2]
        prev_distance = np.sum(np.sqrt(squared_distances))

        locations = s_prime[:-1]
        squared = np.square(locations)
        squared_distances = squared[::2] + squared[1::2]
        new_distance = np.sum(np.sqrt(squared_distances))

        r_agent_distance = prev_distance - new_distance
        # TODO: This is a test
        r_ball_forward = self.ball_location[0] - self.prev_ball[0]
        r_ball_side = abs(self.prev_ball[1]) - abs(self.ball_location[1])
        return r_ball_forward * 5 + r_ball_side * 5  # + r_agent_distance

    params = {
              # general parameters
              'train_every': 1000, 'max_ep_len': 100, 'explore_steps': 2000,

              # reward network
              'reward_width': 300, 'reward_depth': 3, 'reward_lr': 3e-4,
              'reward_batch': 250, 'rotation_invariance': False, 'epochs': 75,
              'noise_std': 0,

              # General Policy parameters
              'policy_epochs': 1, 'a': .2,   # a = lr for q learn

              # cla-specific parameters
              'b': 0, 'boltzmann': 50,

              # diff-q policy gradient parameters
              'q_learn': True, 'gamma': .8, 'td_lambda': .9,

              # control parameters
              # 'rim_size': .02,
              'rim_size': .05,

              # exploration
              'explore': 1, 'explore_decay': .5, 'min_explore': .05,
              }

def to_state_id(robot_id, locations, orientations, ball_location, near_ball):
    """ Generates local, discrete state for robot number {robot_id}

        Indicators: near ball (2), ball location (4), y sign (3), on left (2), on right(2), front(2), back (2)
        Total: 256 states. Lots of them. But, we have lots of robots.

        Simpler: near ball (2), ball location (8), y sign (3)
        Total: 48
        """
    robot_location, robot_ori = locations[robot_id], orientations[robot_id]
    near_ball_indicator = int(distance(robot_location, ball_location) < near_ball)
    ball_location_indicator = determine_direction(robot_location, robot_ori, ball_location)
    ball_y_indicator = 0 if ball_location[1] > 0.05 else (1 if ball_location[1] < -.05 else 2)
    features = [ball_location_indicator, near_ball_indicator, ball_y_indicator]
    # features = [ball_location_indicator, ball_y_indicator]
    state_id = 0
    for i, f in enumerate(features):
        state_id += f * (10 ** i)
    return state_id

number of agents = 8 (even surround constrained)
