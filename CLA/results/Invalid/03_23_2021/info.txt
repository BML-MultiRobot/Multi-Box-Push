    def reward_function(self, s, a, s_prime):
        """ Reward Function using global states and actions """
        s, s_prime = s.flatten(), s_prime.flatten()

        locations = s[:-1]
        squared = np.square(locations)

        squared_distances = squared[::2] + squared[1::2]
        prev_distance = np.sum(np.sqrt(squared_distances))

        locations = s_prime[:-1]
        squared = np.square(locations)
        squared_distances = squared[::2] + squared[1::2]
        new_distance = np.sum(np.sqrt(squared_distances))

        r_ball_forward = self.ball_location[0] - self.prev_ball[0]
        r_ball_side = abs(self.prev_ball[1]) - abs(self.ball_location[1])
        return r_ball_forward * 5 + r_ball_side * 5

    params = {
              # general parameters
              'train_every': 2000, 'max_ep_len': 50, 'explore_steps': 2000, 'test_mode': False,

              # reward network
              'reward_width': 300, 'reward_depth': 3, 'reward_lr': 3e-4,
              'reward_batch': 250, 'rotation_invariance': False, 'epochs': 75,
              'noise_std': 0,

              # General Policy parameters
              'policy_epochs': 1, 'a': .25,   # a = lr for q learn policy gradient

              # cla-specific parameters
              'b': 0, 'boltzmann': 50,

              # diff-q policy gradient parameters
              'q_learn': True, 'q_lr': .1, 'gamma': .99, 'td_lambda': .25, 'alpha': 1, # proportion for reward attribution vs intrinsic
              'steps_per_train': 1,

              # control parameters
              # 'rim_size': .02,
              'rim_size': .05, 'near_ball': .425,

              # exploration
              'explore': 1, 'explore_decay': .85, 'min_explore': .05,
              }


def to_state_id(num_bots, locations, orientations, ball_location, near_ball):
    """ Generates local, discrete state for robot number {robot_id}

        Indicators: near ball (2), ball location (4), y sign (3), on left (2), on right(2), front(2), back (2)
        Total: 256 states. Lots of them. But, we have lots of robots.

        Simpler: near ball (2), ball location (8), y sign (3)
        Total: 48
        """
    features = {i: [] for i in range(num_bots)}
    state_ids = {}

    location_indicators, allowed = {}, {}
    occupied = [False for _ in range(16)]  # Indicators for different positions around ball
    for robot_id in range(num_bots):
        robot_location, robot_ori = locations[robot_id], orientations[robot_id]
        near_ball_indicator = int(distance(robot_location, ball_location) < near_ball)
        state_location_indicator, actual_location_indicator = determine_direction(robot_location, robot_ori, ball_location)
        occupied[actual_location_indicator] = True
        location_indicators[robot_id] = actual_location_indicator
        ball_y_indicator = 0 if ball_location[1] > 0.05 else (1 if ball_location[1] < -.05 else 2)
        features[robot_id] = [state_location_indicator, near_ball_indicator, ball_y_indicator]

    for robot_id in range(num_bots):
        f = features[robot_id]
        loc = location_indicators[robot_id]
        clockwise = (loc - 1) % 16
        c_clockwise = (loc + 1) % 16
        f.append(occupied[clockwise])
        f.append(occupied[c_clockwise])
        allowed[robot_id] = {'c': not occupied[clockwise], 'cc': not occupied[c_clockwise]}

        state_id = 0
        for i, f in enumerate(f):
            state_id += f * (100 ** i)
        state_ids[robot_id] = state_id

    return state_ids, location_indicators, allowed

    def get_advantage(self, s, a, next_s, next_a, robot_ids, p, normalize=False, model=None):
        """ s: n x d array of global states
            a: n x r array of global action indices
            robot_ids: n x 1 array of the agent id in question
            p: n x u array of current automata action probabilities

            Output: beta values for each action (0 to 1), 1 being most favorable """
        p = torch.from_numpy(p)
        output = self.forward_from_numpy_all_actions(s, a, robot_ids)
        expected_value = torch.mean(output, dim=1).unsqueeze(1)# torch.sum(output * p, dim=1).unsqueeze(1)

        if model:
            next_states = model.forward_from_numpy_all_actions(s, a, robot_ids)  # n x u x s
            next_a_repeat = np.expand_dims(next_a, axis=1)  # n x r -> n x 1 x r
            next_a_repeat = np.repeat(next_a_repeat, repeats=self.num_actions, axis=1)  # n x 1 x r -> n x u x r
            next_r_distribution = self.forward_from_numpy_all_actions(next_states, next_a_repeat, robot_ids)  # n x u x u
            next_max_r = torch.max(next_r_distribution, dim=2)[0] # n x u

            curr_max_r = torch.max(output, dim=1)[0].unsqueeze(1) # n x 1
            delta_r = next_max_r - curr_max_r  # n x u

            contribution_r = output - expected_value
            distribution_r = delta_r - torch.mean(delta_r, dim=1).unsqueeze(1)
            # print('min, max distribution: ', torch.min(distribution_r), torch.max(distribution_r),
            #       'min, max contribution: ', torch.min(contribution_r), torch.max(contribution_r))
            advantages = contribution_r + 8 * distribution_r

        else:
            advantages = output - expected_value
        if any(torch.isnan(advantages).flatten()):
            print('NAN DETECTED IN GET_ADVANTAGE: ', output, '#### EXPECTED VALUE ####', expected_value, '### P ###', p)

        if normalize:
            minimum, _ = torch.min(advantages, dim=1)
            minimum = minimum.unsqueeze(1)
            advantages = advantages - minimum + 1

            advantages = torch.exp(advantages * self.boltzmann)

            maximum, _ = torch.max(advantages, dim=1)
            maximum = maximum.unsqueeze(1)

            advantages = advantages / maximum

        robot_ids = torch.from_numpy(robot_ids).unsqueeze(1)
        particular_actions = torch.gather(torch.from_numpy(a), 1, robot_ids)
        advantages = torch.gather(advantages, 1, particular_actions).detach().numpy()

        return advantages.flatten()


number of agents = 8 (allowed 16 subdivision movement and 16 bsubdivision state)
