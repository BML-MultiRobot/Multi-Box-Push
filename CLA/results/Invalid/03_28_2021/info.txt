    def reward_function(self, s, a, s_prime):
        """ Reward Function using global states and actions """
        s, s_prime = s.flatten(), s_prime.flatten()

        locations = s[:-1]
        squared = np.square(locations)

        squared_distances = squared[::2] + squared[1::2]
        prev_distance = np.sum(np.sqrt(squared_distances))

        locations = s_prime[:-1]
        squared = np.square(locations)
        squared_distances = squared[::2] + squared[1::2]
        new_distance = np.sum(np.sqrt(squared_distances))

        r_ball_forward = self.ball_location[0] - self.prev_ball[0]
        r_ball_side = abs(self.prev_ball[1]) - abs(self.ball_location[1])
        return r_ball_forward * 5 + r_ball_side * 5

if __name__ == '__main__':
    rospy.init_node('Dum', anonymous=True)
    num_agents = rospy.get_param('~num_bots')
    params = {
              # general parameters
              'train_every': 2000, 'max_ep_len': 50, 'explore_steps': 2000, 'test_mode': False,

              # reward network
              'reward_width': 300, 'reward_depth': 3, 'reward_lr': 3e-4,
              'reward_batch': 250, 'rotation_invariance': False, 'epochs': 75,
              'noise_std': 0, 'dist_weight': 0,

              # General Policy parameters
              'policy_epochs': 1, 'a': .75,   # a = lr for q learn policy gradient

              # cla-specific parameters
              'b': 0, 'boltzmann': 50,

              # diff-q policy gradient parameters
              'q_learn': True, 'q_lr': .1, 'gamma': .99, 'td_lambda': .25, 'alpha': 1, # proportion for reward attribution vs intrinsic
              'steps_per_train': 1,

              # control parameters
              # 'rim_size': .02,
              'rim_size': .05, 'near_ball': .425,

              # exploration
              'explore': 1, 'explore_decay': .85, 'min_explore': .05,
              }


def to_state_id(num_bots, locations, orientations, ball_location, near_ball):
    """ Generates local, discrete state for robot number {robot_id}

        Indicators: near ball (2), ball location (4), y sign (3), on left (2), on right(2), front(2), back (2)
        Total: 256 states. Lots of them. But, we have lots of robots.

        Simpler: near ball (2), ball location (8), y sign (3)
        Total: 48
        """
    features = {i: [] for i in range(num_bots)}
    state_ids = {}

    location_indicators, allowed = {}, {}
    occupied = [False for _ in range(16)]  # Indicators for different positions around ball
    for robot_id in range(num_bots):
        robot_location, robot_ori = locations[robot_id], orientations[robot_id]
        near_ball_indicator = int(distance(robot_location, ball_location) < near_ball)
        state_location_indicator, actual_location_indicator = determine_direction(robot_location, robot_ori, ball_location)
        occupied[actual_location_indicator] = True
        location_indicators[robot_id] = actual_location_indicator
        ball_y_indicator = 0 if ball_location[1] > 0.05 else (1 if ball_location[1] < -.05 else 2)
        features[robot_id] = [state_location_indicator, near_ball_indicator, ball_y_indicator]

    for robot_id in range(num_bots):
        f = features[robot_id]
        loc = location_indicators[robot_id]
        clockwise = (loc - 1) % 16
        c_clockwise = (loc + 1) % 16
        f.append(occupied[clockwise])
        f.append(occupied[c_clockwise])
        allowed[robot_id] = {'c': not occupied[clockwise], 'cc': not occupied[c_clockwise]}

        state_id = 0
        for i, f in enumerate(f):
            state_id += f * (100 ** i)
        state_ids[robot_id] = state_id

    return state_ids, location_indicators, allowed


number of agents = 8 (allowed 16 subdivision movement and 16 bsubdivision state)
